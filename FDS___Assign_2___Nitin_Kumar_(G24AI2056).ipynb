{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNivrcKucx+5xOsetpGSh7q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nitin5499/Bank-Loan-analysis/blob/main/FDS___Assign_2___Nitin_Kumar_(G24AI2056).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bMTDjAc2uuM",
        "outputId": "45ea43f2-f473-4d7a-c40c-9be637a0912e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mrjob\n",
            "  Downloading mrjob-0.7.4-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.11/dist-packages (from mrjob) (6.0.2)\n",
            "Downloading mrjob-0.7.4-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/439.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m430.1/439.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.6/439.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mrjob\n",
            "Successfully installed mrjob-0.7.4\n",
            "^C\n",
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Error is not recoverable: exiting now\n",
            "Dependencies and Hadoop setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Installation of mrjob and Java for Hadoop\n",
        "!pip install mrjob\n",
        "\n",
        "!apt-get install openjdk-8-jdk -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Downloading and settingup Hadoop\n",
        "HADOOP_VERSION = \"3.3.6\"\n",
        "!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-{HADOOP_VERSION}/hadoop-{HADOOP_VERSION}.tar.gz\n",
        "!tar -xzf hadoop-{HADOOP_VERSION}.tar.gz\n",
        "!mv hadoop-{HADOOP_VERSION} /usr/local/hadoop\n",
        "\n",
        "# Setting up the Hadoop environment variables\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n",
        "os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":\" + os.environ[\"HADOOP_HOME\"] + \"/bin\"\n",
        "\n",
        "# Handle CLASSPATH: Check if it exists before appending, otherwise initialize it.\n",
        "if \"CLASSPATH\" in os.environ:\n",
        "    os.environ[\"CLASSPATH\"] = os.environ[\"CLASSPATH\"] + \":\" + os.environ[\"HADOOP_HOME\"] + \"/lib/*\"\n",
        "else:\n",
        "    os.environ[\"CLASSPATH\"] = os.environ[\"HADOOP_HOME\"] + \"/lib/*\"\n",
        "\n",
        "print(\"Dependencies and Hadoop setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the CSVs--\n",
        "CRUISE_URL = \"https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/cruise.csv\"\n",
        "CHURN_URL = \"https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/customer_churn.csv\"\n",
        "ECOMMERCE_URL = \"https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/e-com_customer.csv\"\n",
        "\n",
        "CRUISE_FILE = \"cruise.csv\"\n",
        "CHURN_FILE = \"customer_churn.csv\"\n",
        "ECOMMERCE_FILE = \"e_commerce_customer.csv\"\n",
        "\n",
        "print(\"Downloading files from provided URLs...\")\n",
        "\n",
        "!wget -O {CRUISE_FILE} {CRUISE_URL}\n",
        "\n",
        "!wget -O {CHURN_FILE} {CHURN_URL}\n",
        "\n",
        "!wget -O {ECOMMERCE_FILE} {ECOMMERCE_URL}\n",
        "\n",
        "print(\"\\nCSV files loaded/prepared!\")\n",
        "!ls -lh *.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JnaJ9BK3FpP",
        "outputId": "441ed798-4c82-4e0e-b6a6-338f32d6beb5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files from provided URLs...\n",
            "--2025-07-28 16:32:12--  https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/cruise.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8734 (8.5K) [text/plain]\n",
            "Saving to: ‘cruise.csv’\n",
            "\n",
            "\rcruise.csv            0%[                    ]       0  --.-KB/s               \rcruise.csv          100%[===================>]   8.53K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-07-28 16:32:12 (77.2 MB/s) - ‘cruise.csv’ saved [8734/8734]\n",
            "\n",
            "--2025-07-28 16:32:12--  https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/customer_churn.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 115479 (113K) [text/plain]\n",
            "Saving to: ‘customer_churn.csv’\n",
            "\n",
            "customer_churn.csv  100%[===================>] 112.77K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-07-28 16:32:12 (11.4 MB/s) - ‘customer_churn.csv’ saved [115479/115479]\n",
            "\n",
            "--2025-07-28 16:32:12--  https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/e-com_customer.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 86871 (85K) [text/plain]\n",
            "Saving to: ‘e_commerce_customer.csv’\n",
            "\n",
            "e_commerce_customer 100%[===================>]  84.83K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-07-28 16:32:12 (8.42 MB/s) - ‘e_commerce_customer.csv’ saved [86871/86871]\n",
            "\n",
            "\n",
            "CSV files loaded/prepared!\n",
            "-rw-r--r-- 1 root root 8.6K Jul 28 16:32 cruise.csv\n",
            "-rw-r--r-- 1 root root 113K Jul 28 16:32 customer_churn.csv\n",
            "-rw-r--r-- 1 root root  85K Jul 28 16:32 e_commerce_customer.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: Cruise Line Aggregations\n",
        "\n",
        "This task involves performing aggregations (e.g., total passengers and tonnage) by cruise line using MapReduce."
      ],
      "metadata": {
        "id": "RBRqVhLW4Kq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cruise_aggregations_job.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv\n",
        "\n",
        "class MRCruiseAggregations(MRJob):\n",
        "    \"\"\"\n",
        "    Computes total ships, average tonnage, and maximum crew size for each cruise line.\n",
        "    Uses a combiner for partial aggregation to improve efficiency.\n",
        "    \"\"\"\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper_get_cruise_data,\n",
        "                   combiner=self.combiner_partial_aggregations,\n",
        "                   reducer=self.reducer_final_aggregations)\n",
        "        ]\n",
        "\n",
        "    def mapper_get_cruise_data(self, _, line):\n",
        "        \"\"\"\n",
        "        Mapper: Parses each line of the cruise CSV.\n",
        "        Emits (Cruise_line, (1, Tonnage, Crew)).\n",
        "        '1' for counting ships, Tonnage for sum, Crew for max.\n",
        "        \"\"\"\n",
        "        # Skip header\n",
        "        if line.startswith(\"Cruise_line\"):\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # Using csv.reader for robust parsing, especially with commas in data\n",
        "            # csv.reader expects an iterable of lines, so [line] wraps the current line.\n",
        "            row = next(csv.reader([line]))\n",
        "            cruise_line = row[0]\n",
        "            # Tonnage is at index 2, crew at index 4 (0-indexed)\n",
        "            tonnage = float(row[2])\n",
        "            crew = int(row[4])\n",
        "            # Emit: (count of ships, sum of tonnage, max crew size)\n",
        "            yield cruise_line, (1, tonnage, crew)\n",
        "        except (ValueError, IndexError) as e:\n",
        "            # Increment a counter for bad lines to monitor data quality\n",
        "            self.increment_counter('MRCruiseAggregations', 'Bad CSV lines', 1)\n",
        "            # print(f\"Skipping malformed line: {line} - Error: {e}\") # Uncomment for debugging\n",
        "            pass\n",
        "\n",
        "    def combiner_partial_aggregations(self, cruise_line, values):\n",
        "        \"\"\"\n",
        "        Combiner: Performs partial aggregation (sum for count/tonnage, max for crew)\n",
        "        before sending to the reducer. This reduces data shuffled across the network.\n",
        "        \"\"\"\n",
        "        total_ships = 0\n",
        "        total_tonnage = 0.0\n",
        "        max_crew = 0\n",
        "\n",
        "        for count, tonnage, crew in values:\n",
        "            total_ships += count\n",
        "            total_tonnage += tonnage\n",
        "            max_crew = max(max_crew, crew)\n",
        "        yield cruise_line, (total_ships, total_tonnage, max_crew)\n",
        "\n",
        "    def reducer_final_aggregations(self, cruise_line, values):\n",
        "        \"\"\"\n",
        "        Reducer: Aggregates the partial results from combiners/mappers.\n",
        "        Computes final total ships, average tonnage, and maximum crew size.\n",
        "        \"\"\"\n",
        "        total_ships = 0\n",
        "        total_tonnage = 0.0\n",
        "        max_crew = 0\n",
        "\n",
        "        for count, tonnage, crew in values:\n",
        "            total_ships += count\n",
        "            total_tonnage += tonnage\n",
        "            max_crew = max(max_crew, crew)\n",
        "\n",
        "        avg_tonnage = total_tonnage / total_ships if total_ships > 0 else 0.0\n",
        "        yield cruise_line, (total_ships, round(avg_tonnage, 2), max_crew)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRCruiseAggregations.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OG6Tb1g4bN5",
        "outputId": "38d0327d-af0e-4aed-ab44-57b31d76df31"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cruise_aggregations_job.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy input file for demo\n",
        "small_cruise_data = \"\"\"Cruise_line,Cruise_ship_name,Tonnage,passengers,crew,built,Inaugural_Date,Years_in_service,Passenger_density,length,cabins\n",
        "AIDA Cruises,AIDAbella,69203,2050,600,2008,2008,12,33.75,252.0,1025\n",
        "AIDA Cruises,AIDAluna,69203,2050,600,2009,2009,11,33.75,252.0,1025\n",
        "Carnival Cruise Line,Carnival Freedom,110000,2974,1150,2007,2007,13,37.00,290.0,1487\n",
        "Carnival Cruise Line,Carnival Horizon,133500,3960,1450,2018,2018,2,33.71,323.0,1980\n",
        "Royal Caribbean,Allure of the Seas,225282,5400,2200,2010,2010,10,41.67,362.0,2700\n",
        "\"\"\"\n",
        "with open(\"small_cruise.csv\", \"w\") as f:\n",
        "    f.write(small_cruise_data)\n",
        "\n",
        "print(\"Running Cruise Aggregations Job on small_cruise.csv (inline test output):\")\n",
        "!python cruise_aggregations_job.py small_cruise.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSSFlih44sdN",
        "outputId": "c26cab5f-89a5-43e7-8988-c04549963e32"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Cruise Aggregations Job on small_cruise.csv (inline test output):\n",
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/cruise_aggregations_job.root.20250728.163849.395535\n",
            "Running step 1 of 1...\n",
            "job output is in /tmp/cruise_aggregations_job.root.20250728.163849.395535/output\n",
            "Streaming final output from /tmp/cruise_aggregations_job.root.20250728.163849.395535/output...\n",
            "\"Carnival Cruise Line\"\t[2, 121750.0, 1450]\n",
            "\"Royal Caribbean\"\t[1, 225282.0, 2200]\n",
            "\"AIDA Cruises\"\t[2, 69203.0, 600]\n",
            "Removing temp directory /tmp/cruise_aggregations_job.root.20250728.163849.395535...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile company_churn_job.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv\n",
        "\n",
        "class MRCompanyChurn(MRJob):\n",
        "    \"\"\"\n",
        "    Computes churn rate for specified companies from customer_churn.csv.\n",
        "    Uses a two-step pipeline:\n",
        "    Step 1: Mapper emits (Company, 'total') and (Company, 'churned')\n",
        "            if the company is in the VIP list from distributed cache.\n",
        "    Step 2: Reducer calculates churn rate (CHURNED / TOTAL) for each company.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define files to be placed in the distributed cache\n",
        "    def configure_args(self):\n",
        "        super(MRCompanyChurn, self).configure_args()\n",
        "        self.add_file_arg('--vip-companies', help='Path to VIP companies list')\n",
        "\n",
        "    def load_vip_companies(self):\n",
        "        \"\"\"\n",
        "        Mapper initialization: Loads VIP company names from the distributed cache.\n",
        "        This runs once per mapper process.\n",
        "        \"\"\"\n",
        "        self.vip_companies = set()\n",
        "        # self.options.vip_companies will contain the path to the file in the distributed cache\n",
        "        if self.options.vip_companies:\n",
        "            with open(self.options.vip_companies, 'r') as f:\n",
        "                for line in f:\n",
        "                    self.vip_companies.add(line.strip())\n",
        "        else:\n",
        "            # This warning appears if --vip-companies is not provided\n",
        "            self.logger.warning(\"No VIP companies file provided. Processing all companies.\")\n",
        "\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper_init=self.load_vip_companies, # This runs once before the mapper starts processing data\n",
        "                   mapper=self.mapper_churn_counts),\n",
        "            MRStep(reducer=self.reducer_churn_rate)\n",
        "        ]\n",
        "\n",
        "    def mapper_churn_counts(self, _, line):\n",
        "        \"\"\"\n",
        "        Mapper: Parses customer churn data.\n",
        "        Emits (Company, 'total_count') for every record.\n",
        "        Emits (Company, 'churned_count') if Churn == 1.\n",
        "        Only processes companies present in the VIP list (if provided via distributed cache).\n",
        "        \"\"\"\n",
        "        if line.startswith(\"Customer ID\"): # Skip header\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            row = next(csv.reader([line]))\n",
        "            # Assuming format: Customer ID,Company,Region,Churn\n",
        "            company = row[1]\n",
        "            churn = int(row[3])\n",
        "\n",
        "            # Filter by VIP companies if the list is loaded, otherwise process all (if vip_companies is empty)\n",
        "            if not self.vip_companies or company in self.vip_companies:\n",
        "                yield company, ('total', 1)\n",
        "                if churn == 1:\n",
        "                    yield company, ('churned', 1)\n",
        "        except (ValueError, IndexError) as e:\n",
        "            self.increment_counter('MRCompanyChurn', 'Bad CSV lines', 1)\n",
        "            # print(f\"Skipping malformed line: {line} - Error: {e}\") # Uncomment for debugging\n",
        "            pass\n",
        "\n",
        "    def reducer_churn_rate(self, company, counts):\n",
        "        \"\"\"\n",
        "        Reducer: Aggregates counts and calculates the churn rate for each company.\n",
        "        \"\"\"\n",
        "        total_customers = 0\n",
        "        churned_customers = 0\n",
        "\n",
        "        for count_type, value in counts:\n",
        "            if count_type == 'total':\n",
        "                total_customers += value\n",
        "            elif count_type == 'churned':\n",
        "                churned_customers += value\n",
        "\n",
        "        churn_rate = 0.0\n",
        "        if total_customers > 0:\n",
        "            churn_rate = float(churned_customers) / total_customers\n",
        "\n",
        "        yield company, f\"{churn_rate:.4f}\" # Output as four-decimal float\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRCompanyChurn.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn5HMgMU5p1g",
        "outputId": "a4f98b7e-bdb8-45f6-a12e-f618e126bd8c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing company_churn_job.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inline Test Output for Cruise Aggregation\n",
        "\n",
        "Below is the result of running the cruise aggregation job on sample input data."
      ],
      "metadata": {
        "id": "dbNE6RkW5yXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vip_companies.txt\n",
        "AlphaCorp\n",
        "BetaCorp\n",
        "DeltaCorp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJTLubUG5zHP",
        "outputId": "aaee9bef-3ad3-46ca-f609-06140264230f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vip_companies.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Customer Churn by Company\n",
        "\n",
        "This task analyzes churned customers grouped by their associated companies using MapReduce."
      ],
      "metadata": {
        "id": "Qk4htp1N5781"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy input file for demo\n",
        "small_churn_data = \"\"\"Customer ID,Company,Region,Churn\n",
        "C001,AlphaCorp,North,0\n",
        "C002,BetaCorp,South,1\n",
        "C003,AlphaCorp,East,1\n",
        "C004,GammaCorp,West,0\n",
        "C005,BetaCorp,North,0\n",
        "C006,AlphaCorp,Central,0\n",
        "C007,BetaCorp,South,1\n",
        "C008,AlphaCorp,South,1\n",
        "C009,DeltaCorp,East,0\n",
        "C010,BetaCorp,West,0\n",
        "\"\"\"\n",
        "with open(\"small_customer_churn.csv\", \"w\") as f:\n",
        "    f.write(small_churn_data)\n",
        "\n",
        "print(\"Running Company Churn Rate Job on small_customer_churn.csv with VIP companies (inline test output):\")\n",
        "# Use --files to put vip_companies.txt into the distributed cache (required by Hadoop)\n",
        "# Use --vip-companies to tell your mrjob script the name of the file it needs to open\n",
        "!python company_churn_job.py small_customer_churn.csv --files vip_companies.txt --vip-companies vip_companies.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTfDJRHW54hN",
        "outputId": "633edb31-4543-40e8-d5c9-503fd161f1cf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Company Churn Rate Job on small_customer_churn.csv with VIP companies (inline test output):\n",
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/company_churn_job.root.20250728.164040.005449\n",
            "Running step 1 of 2...\n",
            "Running step 2 of 2...\n",
            "job output is in /tmp/company_churn_job.root.20250728.164040.005449/output\n",
            "Streaming final output from /tmp/company_churn_job.root.20250728.164040.005449/output...\n",
            "\"BetaCorp\"\t\"0.5000\"\n",
            "\"DeltaCorp\"\t\"0.0000\"\n",
            "\"AlphaCorp\"\t\"0.5000\"\n",
            "Removing temp directory /tmp/company_churn_job.root.20250728.164040.005449...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile state_spending_job.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv\n",
        "import re\n",
        "\n",
        "class MRStateSpending(MRJob):\n",
        "    \"\"\"\n",
        "    Computes total yearly amount spent per state from e-commerce customer data\n",
        "    and then outputs the top 5 states by spending.\n",
        "    \"\"\"\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            # Step 1: Map (extract state & spending) -> Reduce (sum spending per state)\n",
        "            MRStep(mapper=self.mapper_get_state_spending,\n",
        "                   reducer=self.reducer_sum_spending_per_state),\n",
        "            # Step 2: Reduce (collect all sums, sort, and find top 5)\n",
        "            MRStep(reducer=self.reducer_find_top_states)\n",
        "        ]\n",
        "\n",
        "    def mapper_get_state_spending(self, _, line):\n",
        "        \"\"\"\n",
        "        Mapper: Parses the E-commerce Customer CSV, extracts the state code\n",
        "        from the Address, and emits (State, Yearly Amount Spent).\n",
        "        \"\"\"\n",
        "        if line.startswith(\"Email\"): # Skip header line\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            row = next(csv.reader([line]))\n",
        "            # Assuming columns are: Email,Address,Avatar,Avg. Session Length,Time on App,Time on Website,Length of Membership,Yearly Amount Spent\n",
        "            address = row[1] # Address is the second column (index 1)\n",
        "            yearly_amount_spent = float(row[7]) # Yearly Amount Spent is the eighth column (index 7)\n",
        "\n",
        "            # Regex to find a two-letter uppercase state code followed by a 5-digit zip code\n",
        "            # Example: \"123 Main St, Anytown, CA 90210\" -> extracts \"CA\"\n",
        "            match = re.search(r',\\s*([A-Z]{2})\\s*\\d{5}', address)\n",
        "            if match:\n",
        "                state_code = match.group(1) # Extract the two-letter state code\n",
        "                yield state_code, yearly_amount_spent\n",
        "            else:\n",
        "                self.increment_counter('MRStateSpending', 'No state code found', 1)\n",
        "        except (ValueError, IndexError, TypeError) as e:\n",
        "            # Catch errors for malformed lines or incorrect data types\n",
        "            self.increment_counter('MRStateSpending', 'Bad CSV lines', 1)\n",
        "            # print(f\"Skipping malformed line: {line} - Error: {e}\") # Uncomment for debugging\n",
        "            pass\n",
        "\n",
        "    def reducer_sum_spending_per_state(self, state, amounts):\n",
        "        \"\"\"\n",
        "        Reducer 1: Sums the yearly amount spent for each state.\n",
        "        Emits (None, (total_spending, state)) to prepare for a global sort\n",
        "        in the next reducer step.\n",
        "        \"\"\"\n",
        "        total_spending = sum(amounts)\n",
        "        # Emit with a None key so all pairs go to a single reducer in the next step\n",
        "        yield None, (total_spending, state)\n",
        "\n",
        "    def reducer_find_top_states(self, _, spending_state_pairs):\n",
        "        \"\"\"\n",
        "        Reducer 2: Collects all (total_spending, state) pairs, sorts them globally,\n",
        "        and yields only the top 5 states by spending.\n",
        "        \"\"\"\n",
        "        # Collect all (spending, state) tuples and sort in descending order of spending\n",
        "        sorted_states = sorted(spending_state_pairs, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        # Emit only the top 5 states\n",
        "        for i, (total_spending, state) in enumerate(sorted_states):\n",
        "            if i < 5:\n",
        "                yield state, round(total_spending, 2)\n",
        "            else:\n",
        "                break # Stop after emitting the top 5\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRStateSpending.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNzh02766Ez-",
        "outputId": "1e936a68-cd00-46d2-93af-fea64c3cf4d3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing state_spending_job.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inline Test Output for Customer Churn Analysis"
      ],
      "metadata": {
        "id": "i8gj81VL6NbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy input file for demo (with quotes address)\n",
        "small_ecommerce_data = \"\"\"Email,Address,Avatar,Avg. Session Length,Time on App,Time on Website,Length of Membership,Yearly Amount Spent\n",
        "cust1@example.com,\"123 Main St, Springfield, IL 62701\",20.0,3.5,0.0,12.0,500.25\n",
        "cust2@example.com,\"456 Oak Ave, Pleasantville, CA 90210\",30.5,4.0,2.1,24.5,1200.50\n",
        "cust3@example.com,\"789 Pine Ln, Metropolis, NY 10001\",25.1,2.8,1.5,18.0,800.75\n",
        "cust4@example.com,\"101 Elm Blvd, Springfield, IL 62701\",15.0,2.0,0.5,8.0,300.00\n",
        "cust5@example.com,\"202 Maple Dr, Sunnydale, CA 90210\",40.0,5.0,3.0,36.0,1500.00\n",
        "cust6@example.com,\"303 River Rd, Gotham, NY 10001\",22.5,3.1,1.0,15.0,950.00\n",
        "cust7@example.com,\"404 Hilltop, Smallville, KS 66044\",18.0,2.5,0.2,10.0,400.00\n",
        "cust8@example.com,\"505 Valley Dr, Central City, CA 90210\",35.0,4.5,2.5,30.0,1100.00\n",
        "cust9@example.com,\"606 Ocean Ave, Star City, WA 98001\",28.0,3.8,1.8,20.0,1300.00\n",
        "cust10@example.com,\"707 Mountain Rd, Riverdale, NY 10001\",10.0,1.5,0.0,5.0,200.00\n",
        "\"\"\"\n",
        "with open(\"small_e_commerce_customer.csv\", \"w\") as f:\n",
        "    f.write(small_ecommerce_data)\n",
        "\n",
        "print(\"small_e_commerce_customer.csv has been updated with quoted addresses.\")\n",
        "\n",
        "# Optional: Verify the file content by reading it back\n",
        "# with open(\"small_e_commerce_customer.csv\", \"r\") as f:\n",
        "#     print(\"\\nContent of updated small_e_commerce_customer.csv:\")\n",
        "#     print(f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSd4Ac5P6K3b",
        "outputId": "90254ab2-b4d6-4f99-ce3f-418a8960a43b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "small_e_commerce_customer.csv has been updated with quoted addresses.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: E-Commerce Customer Spending by Country\n",
        "\n",
        "This job computes spending insights from an e-commerce dataset using MapReduce."
      ],
      "metadata": {
        "id": "wJVI6A376hTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running State-wise Spending Job on small_e_commerce_customer.csv (inline test output):\")\n",
        "!python state_spending_job.py small_e_commerce_customer.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_91URtg6kpQ",
        "outputId": "9090fea6-8d18-425c-97ad-e99fb711eddc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running State-wise Spending Job on small_e_commerce_customer.csv (inline test output):\n",
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/state_spending_job.root.20250728.164303.702416\n",
            "Running step 1 of 2...\n",
            "\n",
            "Counters: 1\n",
            "\tMRStateSpending\n",
            "\t\tBad CSV lines=10\n",
            "\n",
            "Running step 2 of 2...\n",
            "job output is in /tmp/state_spending_job.root.20250728.164303.702416/output\n",
            "Streaming final output from /tmp/state_spending_job.root.20250728.164303.702416/output...\n",
            "Removing temp directory /tmp/state_spending_job.root.20250728.164303.702416...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy input file for demo (with quoted addess and complete columns)\n",
        "small_ecommerce_data = \"\"\"Email,Address,Avatar,Avg. Session Length,Time on App,Time on Website,Length of Membership,Yearly Amount Spent\n",
        "cust1@example.com,\"123 Main St, Springfield, IL 62701\",Lavender,20.0,3.5,0.0,12.0,500.25\n",
        "cust2@example.com,\"456 Oak Ave, Pleasantville, CA 90210\",Teal,30.5,4.0,2.1,24.5,1200.50\n",
        "cust3@example.com,\"789 Pine Ln, Metropolis, NY 10001\",Blue,25.1,2.8,1.5,18.0,800.75\n",
        "cust4@example.com,\"101 Elm Blvd, Springfield, IL 62701\",Green,15.0,2.0,0.5,8.0,300.00\n",
        "cust5@example.com,\"202 Maple Dr, Sunnydale, CA 90210\",Red,40.0,5.0,3.0,36.0,1500.00\n",
        "cust6@example.com,\"303 River Rd, Gotham, NY 10001\",Orange,22.5,3.1,1.0,15.0,950.00\n",
        "cust7@example.com,\"404 Hilltop, Smallville, KS 66044\",Yellow,18.0,2.5,0.2,10.0,400.00\n",
        "cust8@example.com,\"505 Valley Dr, Central City, CA 90210\",Purple,35.0,4.5,2.5,30.0,1100.00\n",
        "cust9@example.com,\"606 Ocean Ave, Star City, WA 98001\",Gray,28.0,3.8,1.8,20.0,1300.00\n",
        "cust10@example.com,\"707 Mountain Rd, Riverdale, NY 10001\",Black,10.0,1.5,0.0,5.0,200.00\n",
        "\"\"\"\n",
        "with open(\"small_e_commerce_customer.csv\", \"w\") as f:\n",
        "    f.write(small_ecommerce_data)\n",
        "\n",
        "print(\"small_e_commerce_customer.csv has been updated with complete, quoted data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpa9PYZY6n7G",
        "outputId": "d5339ecd-b60e-448b-8dd2-986535075938"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "small_e_commerce_customer.csv has been updated with complete, quoted data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inline Test Output for E-Commerce Spending Analysis"
      ],
      "metadata": {
        "id": "lFDiu7r-66mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ship_filter_median_length_job.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv\n",
        "import math\n",
        "\n",
        "class MRShipFilterMedianLength(MRJob):\n",
        "    \"\"\"\n",
        "    A two-step MapReduce pipeline on cruise.csv:\n",
        "    Step 1: Filters ships with passenger density > 35.0 and emits (Cruise line, length).\n",
        "    Step 2: Computes the median of the lengths for each Cruise line.\n",
        "    Handles both even and odd counts for median calculation.\n",
        "    \"\"\"\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper_filter_ships,\n",
        "                   # No combiner here as we need all lengths per key for median calculation\n",
        "                   reducer=self.reducer_compute_median_length)\n",
        "        ]\n",
        "\n",
        "    def mapper_filter_ships(self, _, line):\n",
        "        \"\"\"\n",
        "        Mapper: Filters ships based on passenger density.\n",
        "        Emits (Cruise_line, length) for ships with passenger density > 35.0.\n",
        "        \"\"\"\n",
        "        if line.startswith(\"Cruise_line\"): # Skip header\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # Using csv.reader for robust parsing\n",
        "            row = next(csv.reader([line]))\n",
        "            cruise_line = row[0]\n",
        "            passenger_density = float(row[8]) # Passenger_density is 9th column (index 8)\n",
        "            length = float(row[9]) # length is 10th column (index 9)\n",
        "\n",
        "            if passenger_density > 35.0:\n",
        "                yield cruise_line, length\n",
        "        except (ValueError, IndexError) as e:\n",
        "            self.increment_counter('MRShipFilterMedianLength', 'Bad CSV lines', 1)\n",
        "            # print(f\"Skipping malformed line: {line} - Error: {e}\") # Uncomment for debugging\n",
        "            pass\n",
        "\n",
        "    def reducer_compute_median_length(self, cruise_line, lengths):\n",
        "        \"\"\"\n",
        "        Reducer: Computes the median length for each cruise line.\n",
        "        Handles even and odd counts for median calculation.\n",
        "        \"\"\"\n",
        "        # Collect all lengths and sort them to find the median\n",
        "        all_lengths = sorted(list(lengths))\n",
        "        n = len(all_lengths)\n",
        "\n",
        "        median = 0.0\n",
        "        if n == 0:\n",
        "            median = 0.0 # No lengths found for this cruise line after filtering\n",
        "        elif n % 2 == 1: # Odd number of elements\n",
        "            median = all_lengths[n // 2]\n",
        "        else: # Even number of elements\n",
        "            mid1 = all_lengths[n // 2 - 1]\n",
        "            mid2 = all_lengths[n // 2]\n",
        "            median = (mid1 + mid2) / 2.0\n",
        "\n",
        "        yield cruise_line, round(median, 2)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRShipFilterMedianLength.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjzGTN6P66Wm",
        "outputId": "25c2a283-ff03-41c4-e255-54377be505bd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ship_filter_median_length_job.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_cruise_data = \"\"\"Cruise_line,Cruise_ship_name,Tonnage,passengers,crew,built,Inaugural_Date,Years_in_service,Passenger_density,length,cabins\n",
        "AIDA Cruises,AIDAbella,69203,2050,600,2008,2008,12,33.75,252.0,1025     # Density 33.75 <= 35.0 -> FILTERED OUT\n",
        "AIDA Cruises,AIDAluna,69203,2050,600,2009,2009,11,33.75,252.0,1025      # Density 33.75 <= 35.0 -> FILTERED OUT\n",
        "Carnival Cruise Line,Carnival Freedom,110000,2974,1150,2007,2007,13,37.00,290.0,1487 # Density 37.00 > 35.0 -> KEPT (length 290.0)\n",
        "Carnival Cruise Line,Carnival Horizon,133500,3960,1450,2018,2018,2,33.71,323.0,1980 # Density 33.71 <= 35.0 -> FILTERED OUT\n",
        "Royal Caribbean,Allure of the Seas,225282,5400,2200,2010,2010,10,41.67,362.0,2700 # Density 41.67 > 35.0 -> KEPT (length 362.0)\n",
        "\"\"\"\n",
        "# The small_cruise.csv file should already exist from previous questions.\n",
        "# If not, run the cell that defines and writes it in section 1.\n",
        "# Or explicitly create it again if you're only testing this part:\n",
        "with open(\"small_cruise.csv\", \"w\") as f:\n",
        "    f.write(small_cruise_data)\n",
        "\n",
        "print(\"Running Ship Filter & Median Length Job on small_cruise.csv (inline test output):\")\n",
        "!python ship_filter_median_length_job.py small_cruise.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_i2tNSX63u_",
        "outputId": "b47d8885-a966-41d9-f095-5506e5665dd0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Ship Filter & Median Length Job on small_cruise.csv (inline test output):\n",
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/ship_filter_median_length_job.root.20250728.164503.676263\n",
            "Running step 1 of 1...\n",
            "job output is in /tmp/ship_filter_median_length_job.root.20250728.164503.676263/output\n",
            "Streaming final output from /tmp/ship_filter_median_length_job.root.20250728.164503.676263/output...\n",
            "\"Royal Caribbean\"\t362.0\n",
            "\"Carnival Cruise Line\"\t290.0\n",
            "Removing temp directory /tmp/ship_filter_median_length_job.root.20250728.164503.676263...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X6wn78Rs7FNv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}